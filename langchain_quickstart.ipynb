{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e96ac19",
   "metadata": {},
   "source": [
    "In this quickstart we'll show you how to:\n",
    "\n",
    "* Get setup with LangChain, LangSmith and LangServe\n",
    "* Use the most basic and common components of LangChain: prompt templates, models, and output parsers\n",
    "* Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining\n",
    "* Build a simple application with LangChain\n",
    "* Trace your application with LangSmith\n",
    "* Serve your application with LangServe\n",
    "\n",
    "\n",
    "# Setup\n",
    "\n",
    "## Installation\n",
    "\n",
    "To install LangChain run:\n",
    "\n",
    "# Building with LangChain\n",
    "\n",
    "LangChain enables building application that connect external sources of data and computation to LLMs. In this quickstart, we will walk through a few different ways of doing that. We will start with a simple LLM chain, which just relies on information in the prompt template to respond. Next, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain. This allows you to interact in a chat manner with this LLM, so it remembers previous questions. Finally, we will build an agent - which utilizes an LLM to determine whether or not it needs to fetch data to answer questions. We will cover these at a high level, but there are lot of details to all of these! We will link to relevant docs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e25449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (0.1.17)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.29)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.36)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (0.1.50)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (0.1.53)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langchain-core<0.2.0,>=0.1.48->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46f01017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接本地部署的 Ollama，并使用 Llama2 模型\n",
    "# PS：需要现在本机部署好 Llama2 / Llama3\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0069c1",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [Ollama](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.ollama.Ollama.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33554ba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith is a tool that can assist with testing by providing features such as:\\n\\n1. Code Analysis: Langsmith can analyze your codebase and identify potential issues, such as vulnerabilities, performance bottlenecks, and security risks.\\n2. Automated Testing: Langsmith can automatically test your codebase using a variety of testing frameworks and libraries, including Selenium, JUnit, and Pytest.\\n3. Code Refactoring: Langsmith can suggest improvements to your codebase, such as renaming variables, consolidating redundant code, and improving code organization.\\n4. Dependency Management: Langsmith can help you manage dependencies between different parts of your codebase, ensuring that they are consistent and up-to-date.\\n5. Collaboration Tools: Langsmith provides tools for collaboration, such as real-time commenting and feedback, and the ability to assign tasks and track progress.\\n6. Integration with CI/CD Pipelines: Langsmith can be integrated with your Continuous Integration/Continuous Deployment (CI/CD) pipelines, allowing you to automatically test and deploy your code changes.\\n7. Customizable: Langsmith is highly customizable, allowing you to tailor it to your specific testing needs and workflows.\\n8. Extensive Reporting: Langsmith provides extensive reporting capabilities, including the ability to generate reports on code quality, testing results, and deployment history.\\n9. Support for Multiple Programming Languages: Langsmith supports multiple programming languages, including Java, Python, Ruby, and JavaScript.\\n10. Open-source: Langsmith is open-source, which means that it is free to use and can be customized to fit your specific needs.\\n\\nBy leveraging these features, Langsmith can help automate and streamline the testing process, improving the overall quality and reliability of your codebase.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接给LLM发送问题，不做任何prompt，embedding和context\n",
    "\n",
    "llm.invoke(\"how can langsmith help with testing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99167095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入聊天对话的 Prompt Template，让LLM更加聚焦\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd76313",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f78e5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 Chat Prompt Template和LLM做一个Chain关联\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e3524d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAs a world-class technical documentation writer, I must say that Langsmith is a powerful tool that can greatly assist in the testing process. Here are some ways Langsmith can help:\\n\\n1. Automated Testing: Langsmith's AI-powered engine can automatically generate test cases based on your technical documentation. This can save you a tremendous amount of time and effort compared to writing test cases manually. With Langsmith, you can create comprehensive test suites that cover all aspects of your product, including functionality, performance, security, and usability.\\n2. Test Case Prioritization: Langsmith's AI algorithms can prioritize test cases based on risk and complexity. This ensures that the most critical tests are run first, reducing the overall testing time and effort. By automating the test case prioritization process, you can free up your testing resources for more complex and higher-risk tests.\\n3. Defect Prediction: Langsmith's AI models can analyze your technical documentation to predict potential defects in your product. By identifying potential issues early on, you can fix them before they become bigger problems down the line. This not only reduces the risk of launching a defective product but also helps you deliver a higher-quality product to your customers.\\n4. Test Data Management: Langsmith's AI-powered engine can help manage test data more efficiently. By automating the process of creating and updating test data, you can reduce the risk of manual errors and improve the overall testing process. With Langsmith, you can ensure that your test data is accurate, up-to-date, and relevant to your product's requirements.\\n5. Continuous Integration/Continuous Deployment (CI/CD): Langsmith can integrate seamlessly with your CI/CD pipeline, enabling you to automate the testing process throughout your development lifecycle. By using Langsmith in combination with CI/CD tools like Jenkins or CircleCI, you can streamline your testing process and reduce the time it takes to deliver high-quality software to your customers.\\n6. Customizable Workflows: Langsmith's modular architecture allows you to customize your workflow to fit your specific needs. Whether you need to create a comprehensive test suite for a complex product or integrate with multiple development tools, Langsmith can be tailored to meet your requirements.\\n7. Collaboration and Sharing: Langsmith provides an intuitive interface that allows developers, testers, and technical writers to collaborate and share knowledge more effectively. By using Langsmith's collaboration features, you can streamline your testing process and reduce the risk of miscommunications or misunderstandings between team members.\\n8. Knowledge Management: Langsmith's AI-powered engine can analyze your technical documentation to identify patterns, trends, and insights that can inform future testing strategies. By leveraging Langsmith's knowledge management capabilities, you can gain a deeper understanding of your product's functionality and identify potential areas for improvement.\\n9. Adaptive Testing: Langsmith's AI algorithms can adapt to changing requirements and product updates in real-time. This ensures that your testing process remains up-to-date and relevant to your product's current state. By using Langsmith, you can reduce the risk of missing critical issues or wasting time on unnecessary tests.\\n10. Improved Test Coverage: With Langsmith, you can achieve comprehensive test coverage by automatically generating test cases based on your technical documentation. This ensures that all aspects of your product are thoroughly tested and validated before launch, reducing the risk of defects or failures in the field.\\n\\nIn summary, Langsmith can significantly enhance the testing process by automating test case creation, prioritization, defect prediction, data management, continuous integration/continuous deployment, customizable workflows, collaboration, knowledge management, adaptive testing, and improved test coverage. By leveraging these features, you can deliver high-quality products more efficiently and effectively.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 带上Chat Prompt Template 再次给LLM发送问题，此时LLM仍然不知道问题的答案，但能够用更加合适的\n",
    "# 语气和措辞（as a world class technical documentation writer）来回答\n",
    "\n",
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7013e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将LLM返回结果为一个String方便后续处理\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f58379",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [StrOutputParser](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4bae83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b9e0c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAs a world-class technical documentation writer, I must say that Langsmith is a versatile tool that can greatly assist with testing. Here are some ways in which Langsmith can help:\\n\\n1. Automated Documentation Testing: Langsmith allows you to create automated tests for your technical documents using AI-powered language analysis. This helps ensure that your documentation is accurate, consistent, and up-to-date, which is crucial for effective testing.\\n2. Customizable Test Cases: With Langsmith, you can create custom test cases tailored to your specific documentation needs. This enables you to thoroughly test your documents and identify any potential issues before they become problems.\\n3. Integration with Test Management Tools: Langsmith integrates seamlessly with popular test management tools like JIRA, TestRail, and PractiTest. This integration allows you to easily manage and organize your testing process, streamlining the entire testing workflow.\\n4. Intelligent Review and Feedback: Langsmith's AI-powered language analysis can provide intelligent review and feedback on your technical documents. This helps identify any errors or inconsistencies in the documentation, ensuring that it meets the required standards and is ready for release.\\n5. Documentation Quality Analysis: Langsmith can analyze the quality of your technical documentation by assessing factors like readability, consistency, and accuracy. This enables you to identify areas for improvement and optimize your documentation process accordingly.\\n6. Time-Saving: With Langsmith, you can save time and effort in testing by automating many of the repetitive tasks involved in the testing process. This allows you to focus on more critical aspects of documentation and testing.\\n7. Scalability: Langsmith is designed to handle large volumes of technical documentation, making it an ideal solution for organizations with extensive documentation needs. Whether you're working on a small project or a massive software development effort, Langsmith can scale to meet your needs.\\n8. Cost-Effective: By automating many of the testing tasks, Langsmith can help reduce the overall cost of technical documentation and testing. This is especially important for organizations with limited budgets or resources.\\n9. Enhanced Collaboration: With Langsmith, you can streamline collaboration between team members by providing a centralized platform for document management and testing. This helps ensure that everyone is on the same page and working towards the same goals.\\n10. Continuous Improvement: Langsmith's AI-powered language analysis can help identify areas for continuous improvement in your technical documentation. By analyzing your documentation regularly, you can ensure that it remains up-to-date and relevant to your target audience.\\n\\nIn conclusion, Langsmith is a powerful tool that can greatly assist with testing technical documents. Its automated testing capabilities, customizable test cases, integration with popular test management tools, intelligent review and feedback, documentation quality analysis, scalability, cost-effectiveness, enhanced collaboration, and continuous improvement features make it an ideal solution for organizations looking to streamline their technical documentation and testing processes.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d503f",
   "metadata": {},
   "source": [
    "## Diving Deeper\n",
    "\n",
    "至此，已经成功的通过 LangChain python Lib代码拼凑一个基本的LLM Chain，这里只涉及到了最基本的prompts，模型和输出格式parsers。\n",
    "\n",
    "这里没有涉及到：\n",
    "\n",
    "* Chuning(text splitting, 业界英文环境普遍使用Ada_200模型)\n",
    "* Embedding\n",
    "* Indexing\n",
    "* Vector Store\n",
    "* Retrival\n",
    "\n",
    "等RAG中每个环节需要涉及到的技术细节。\n",
    "\n",
    "[For a deeper dive](https://python.langchain.com/docs/modules/model_io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf18102",
   "metadata": {},
   "source": [
    "# Retrieval Chain\n",
    "\n",
    "To properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM. We can do this via `retrieval`. Retrieval is useful when you have **too much data** to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.\n",
    "\n",
    "In this process, we will look up relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see [this documentation](https://python.langchain.com/docs/modules/data_connection/vectorstores/).\n",
    "\n",
    "First, we need to load the data that we want to index. To do this, we will use the WebBaseLoader. This requires installing [BeautifulSoup:](https://beautiful-soup-4.readthedocs.io/en/latest/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a031ffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4) (2.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fddeb6",
   "metadata": {},
   "source": [
    "After that, we can import and use WebBaseLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ac6fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从互联网Load一篇文章\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043e1f6",
   "metadata": {},
   "source": [
    "API Reference:\n",
    " * [WebBaseLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfb16e",
   "metadata": {},
   "source": [
    "Next, we need to index it into a vectorstore. This requires a few components, namely an [embedding model](https://python.langchain.com/docs/modules/data_connection/text_embedding/) and a [vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores/).\n",
    "\n",
    "For embedding models, we once again provide examples for accessing via API or by running local models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25527268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里使用本地部署的Ollama的embedding接口\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80411014",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [OllamaEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.ollama.OllamaEmbeddings.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253074ae",
   "metadata": {},
   "source": [
    "Now, we can use this embedding model to ingest documents into a vectorstore. We will use a simple local vectorstore, [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss/), for simplicity's sake.\n",
    "\n",
    "First we need to install the required packages for that:\n",
    "\n",
    "在将 Embedding后的向量写入向量库前，为简单起见，入FAISS做本地内存向量存储\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "10a1bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in /Users/li.luo/Library/Python/3.9/lib/python/site-packages (from faiss-cpu) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4b967f",
   "metadata": {},
   "source": [
    "Then we can build our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "270e1ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对刚才的文章进行 embedding（向量化），然后写入 本地 FAISS 向量库中。\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed345cd9",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [FAISS](https://api.python.langchain.com/en/latest/vectorstores/langchain_community.vectorstores.faiss.FAISS.html)\n",
    "* [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5fbd97",
   "metadata": {},
   "source": [
    "Now that we have this data indexed in a vectorstore, we will create a retrieval chain. This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.\n",
    "\n",
    "First, let's set up the chain that takes a question and the retrieved documents and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2538edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有了原始文章的向量索引后，就可以拼凑一个问答式的 Prompt，并结合已经建好的向量索引\n",
    "# 根据问题本身的向量索引和文档向量索引的距离进行向量搜索\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733d777",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [create_stuff_documents_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e567c",
   "metadata": {},
   "source": [
    "If we wanted to, we could run this ourselves by passing in documents directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "168bbae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, Langsmith can help with testing by allowing users to visualize their test results.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00670d97",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [Document](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de28cfe",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just set up. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f667ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bdd19",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [create_retrieval_chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c697e232",
   "metadata": {},
   "source": [
    "We can now invoke this chain. This returns a dictionary - the response from the LLM is in the answer key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29eb7028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the provided context, LangSmith can help with testing in several ways:\n",
      "\n",
      "1. Creation of test cases: LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, that can be used to run tests on their LLM applications.\n",
      "2. Annotation of traces: LangSmith supports sending runs to annotation queues, where annotators can closely inspect interesting traces and annotate them with respect to different criteria. This helps in catching regressions across important evaluation criteria.\n",
      "3. Monitoring and A/B testing: LangSmith provides monitoring charts that allow users to track key metrics over time. Users can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period, which is helpful for debugging production issues.\n",
      "4. Automations: LangSmith allows users to perform actions on traces in near real-time, such as automatically scoring traces, sending them to annotation queues, or sending them to datasets. This can be used to process and score production traces in near real-time.\n",
      "5. Threads view: LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate an application across multiple turns. This is helpful for testing changes in prompt, model, or retrieval strategy.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# LangSmith offers several features that can help with testing:..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546ccb3",
   "metadata": {},
   "source": [
    "This answer should be much more accurate!\n",
    "\n",
    "## Diving Deeper\n",
    "We've now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see [this section of documentation](https://python.langchain.com/docs/modules/data_connection/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3110ecd8",
   "metadata": {},
   "source": [
    "# Conversation Retrieval Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07651578",
   "metadata": {},
   "source": [
    "The chain we've created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?\n",
    "\n",
    "We can still use the `create_retrieval_chain` function, but we need to change two things:\n",
    "\n",
    "1. The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.\n",
    "1. The final LLM chain should likewise take the whole history into account\n",
    "\n",
    "**Updating Retrieval**\n",
    "\n",
    "In order to update retrieval, we will create a new chain. This chain will take in the most recent input (`input`) and the conversation history (`chat_history`) and use an LLM to generate a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73919391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebadef",
   "metadata": {},
   "source": [
    "API Reference:\n",
    " * [create_history_aware_retriever](https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html)\n",
    " * [MessagesPlaceholder](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd3b86",
   "metadata": {},
   "source": [
    "We can test this out by passing in an instance where the user asks a follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b81d9999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Every playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing\\u200bBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it’s important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it’s breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback\\u200bWhen launching your application to an initial set of users, it’s important to gather human feedback on the responses it’s producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces\\u200bLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset\\u200bAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production\\u200bClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you’ll also want to do once your app hits production.However, especially at the production stage, it’s crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Online evaluations and automations allow you to process and score production traces in near real-time.Additionally, threads provide a seamless way to group traces from a single conversation, making it easier to track the performance of your application across multiple turns.Monitoring and A/B Testing\\u200bLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period — this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Automations\\u200bAutomations are a powerful feature in LangSmith that allow you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.To define an automation, simply provide a filter condition, a sampling rate, and an action to perform. Automations are particularly helpful for processing traces at production scale.Threads\\u200bMany LLM applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to\", metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}),\n",
       " Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.Prototyping\\u200bPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing — and debug where it is failing — is incredibly important for this phase.Debugging\\u200bWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn’t necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it’s extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\\u200bWhile many developers still ship an initial version of their application based on “vibe checks”, we’ve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View\\u200bWhen prototyping different versions of your applications and making changes, it’s important to see whether or not you’ve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it’s useful to be able to view results for different configurations on the same datapoints side-by-side. We’ve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground\\u200bLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}),\n",
       " Document(page_content='meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate your application across multiple turns.Was this page helpful?PreviousQuick StartNextOverviewPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}),\n",
       " Document(page_content='LangSmith User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40beb42d",
   "metadata": {},
   "source": [
    "API Reference:\n",
    " * [HumanMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.human.HumanMessage.html)\n",
    " * [AIMessage](https://api.python.langchain.com/en/latest/messages/langchain_core.messages.ai.AIMessage.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04972f1a",
   "metadata": {},
   "source": [
    "You should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow-up question.\n",
    "\n",
    "Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1e563e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42242042",
   "metadata": {},
   "source": [
    "We can now test this out end-to-end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49ebdb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangSmith help test my LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content=\"Every playground run is logged in the system and can be used to create test cases or compare with other runs.Beta Testing\\u200bBeta testing allows developers to collect more data on how their LLM applications are performing in real-world scenarios. In this phase, it’s important to develop an understanding for the types of inputs the app is performing well or poorly on and how exactly it’s breaking down in those cases. Both feedback collection and run annotation are critical for this workflow. This will help in curation of test cases that can help track regressions/improvements and development of automatic evaluations.Capturing Feedback\\u200bWhen launching your application to an initial set of users, it’s important to gather human feedback on the responses it’s producing. This helps draw attention to the most interesting runs and highlight edge cases that are causing problematic responses. LangSmith allows you to attach feedback scores to logged traces (oftentimes, this is hooked up to a feedback button in your app), then filter on traces that have a specific feedback tag and score. A common workflow is to filter on traces that receive a poor user feedback score, then drill down into problematic points using the detailed trace view.Annotating Traces\\u200bLangSmith also supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or even subject matter experts. This allows users to catch regressions across important evaluation criteria.Adding Runs to a Dataset\\u200bAs your application progresses through the beta testing phase, it's essential to continue collecting data to refine and improve its performance. LangSmith enables you to add runs as examples to datasets (from both the project page and within an annotation queue), expanding your test coverage on real-world scenarios. This is a key benefit in having your logging system and your evaluation/testing system in the same platform.Production\\u200bClosely inspecting key data points, growing benchmarking datasets, annotating traces, and drilling down into important data in trace view are workflows you’ll also want to do once your app hits production.However, especially at the production stage, it’s crucial to get a high-level overview of application performance with respect to latency, cost, and feedback scores. This ensures that it's delivering desirable results at scale.Online evaluations and automations allow you to process and score production traces in near real-time.Additionally, threads provide a seamless way to group traces from a single conversation, making it easier to track the performance of your application across multiple turns.Monitoring and A/B Testing\\u200bLangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period — this is especially handy for debugging production issues.LangSmith also allows for tag and metadata grouping, which allows users to mark different versions of their applications with different identifiers and view how they are performing side-by-side within each chart. This is helpful for A/B testing changes in prompt, model, or retrieval strategy.Automations\\u200bAutomations are a powerful feature in LangSmith that allow you to perform actions on traces in near real-time. This can be used to automatically score traces, send them to annotation queues, or send them to datasets.To define an automation, simply provide a filter condition, a sampling rate, and an action to perform. Automations are particularly helpful for processing traces at production scale.Threads\\u200bMany LLM applications are multi-turn, meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to\", metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}),\n",
       "  Document(page_content='Skip to main contentLangSmith API DocsSearchGo to AppQuick StartUser GuideTracingEvaluationProduction Monitoring & AutomationsPrompt HubProxyPricingSelf-HostingCookbookUser GuideOn this pageLangSmith User GuideLangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.Prototyping\\u200bPrototyping LLM applications often involves quick experimentation between prompts, model types, retrieval strategy and other parameters.\\nThe ability to rapidly understand how the model is performing — and debug where it is failing — is incredibly important for this phase.Debugging\\u200bWhen developing new LLM applications, we suggest having LangSmith tracing enabled by default.\\nOftentimes, it isn’t necessary to look at every single trace. However, when things go wrong (an unexpected end result, infinite agent loop, slower than expected execution, higher than expected token usage), it’s extremely helpful to debug by looking through the application traces. LangSmith gives clear visibility and debugging information at each step of an LLM sequence, making it much easier to identify and root-cause issues.\\nWe provide native rendering of chat messages, functions, and retrieve documents.Initial Test Set\\u200bWhile many developers still ship an initial version of their application based on “vibe checks”, we’ve seen an increasing number of engineering teams start to adopt a more test driven approach. LangSmith allows developers to create datasets, which are collections of inputs and reference outputs, and use these to run tests on their LLM applications.\\nThese test cases can be uploaded in bulk, created on the fly, or exported from application traces. LangSmith also makes it easy to run custom evaluations (both LLM and heuristic based) to score test results.Comparison View\\u200bWhen prototyping different versions of your applications and making changes, it’s important to see whether or not you’ve regressed with respect to your initial test cases.\\nOftentimes, changes in the prompt, retrieval strategy, or model choice can have huge implications in responses produced by your application.\\nIn order to get a sense for which variant is performing better, it’s useful to be able to view results for different configurations on the same datapoints side-by-side. We’ve invested heavily in a user-friendly comparison view for test runs to track and diagnose regressions in test scores across multiple revisions of your application.Playground\\u200bLangSmith provides a playground environment for rapid iteration and experimentation.\\nThis allows you to quickly test out different prompts and models. You can open the playground from any prompt or model run in your trace.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}),\n",
       "  Document(page_content='meaning that they involve a series of interactions between the user and the application. LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of and annotate your application across multiple turns.Was this page helpful?PreviousQuick StartNextOverviewPrototypingBeta TestingProductionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2024 LangChain, Inc.', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'}),\n",
       "  Document(page_content='LangSmith User Guide | 🦜️🛠️ LangSmith', metadata={'source': 'https://docs.smith.langchain.com/user_guide', 'title': 'LangSmith User Guide | 🦜️🛠️ LangSmith', 'description': 'LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, we’ll highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if they’re just starting their journey.', 'language': 'en'})],\n",
       " 'answer': 'Of course! LangSmith is designed to help you test and evaluate your LLM applications effectively. Here are some ways LangSmith can assist you in testing your LLM applications:\\n\\n1. Datasets: LangSmith allows you to create datasets, which are collections of inputs and reference outputs, that you can use to run tests on your LLM applications. You can upload these test cases in bulk, create them on the fly, or export them from application traces.\\n2. Test Cases: LangSmith makes it easy to run custom evaluations (both LLM and heuristic-based) to score test results. This helps you track regressions across different evaluation criteria and identify areas for improvement.\\n3. Annotation Queues: LangSmith supports sending runs to annotation queues, which allow annotators to closely inspect interesting traces and annotate them with respect to different criteria. Annotators can be PMs, engineers, or subject matter experts, which helps catch regressions across important evaluation criteria.\\n4. Playground Environment: LangSmith provides a playground environment for rapid iteration and experimentation. You can quickly test out different prompts and models without having to set up a separate testing environment.\\n5. Threads View: LangSmith provides a threads view that groups traces from a single conversation together, making it easier to track the performance of your application across multiple turns. This is particularly helpful for A/B testing changes in prompt, model, or retrieval strategy.\\n6. Monitoring and Automations: LangSmith provides monitoring charts that allow you to track key metrics over time. You can expand to view metrics for a given period and drill down into a specific data point to get a trace table for that time period — this is especially handy for debugging production issues. Additionally, LangSmith allows you to perform actions on traces in near real-time using automations, which can help you quickly identify and address any issues that may arise.\\n\\nOverall, LangSmith provides a comprehensive set of tools to help you test and evaluate your LLM applications effectively. By leveraging these features, you can ensure that your application is performing as expected and make data-driven decisions to improve its performance.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c35442",
   "metadata": {},
   "source": [
    "We can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf1ba7",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "We've so far created examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take.\n",
    "\n",
    "**NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.**\n",
    "\n",
    "One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access to two tools:\n",
    "\n",
    "1. The retriever we just created. This will let it easily answer questions about LangSmith\n",
    "1. A search tool. This will let it easily answer questions that require up-to-date information.\n",
    "\n",
    "First, let's set up a tool for the retriever we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971f6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50e0eb",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [create_retriever_tool](https://api.python.langchain.com/en/latest/tools/langchain_core.tools.create_retriever_tool.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3315f2a",
   "metadata": {},
   "source": [
    "The search tool that we will use is [Tavily](https://python.langchain.com/docs/integrations/retrievers/tavily/). This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:\n",
    "\n",
    "```bash\n",
    "export TAVILY_API_KEY=...\n",
    "```\n",
    "\n",
    "If you do not want to set up an API key, you can skip creating this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "368f540a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32445533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd877486",
   "metadata": {},
   "source": [
    "API Reference:\n",
    "* [TavilySearchResults](https://api.python.langchain.com/en/latest/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a299f706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
